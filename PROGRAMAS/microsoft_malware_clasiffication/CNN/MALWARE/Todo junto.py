"""
Created on Thu Feb  1 18:02:41 2024

@author: pjime
"""
from math import floor, sqrt
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import csv
import os                                             
import multiprocessing as mp
import pickle
from time import perf_counter
import cv2
import math
import tensorflow as tf                                # Importamos TensorFlow, una biblioteca para aprendizaje autom√°tico                                            # Importamos el m√≥dulo os para interactuar con el sistema operativo
from sklearn.model_selection import train_test_split   # Importamos train_test_split para dividir los datos en conjuntos de entrenamiento y prueba
from keras.utils import to_categorical                 # Importamos to_categorical para codificar las etiquetas en formato one-hot
from keras.models import Sequential                   # Importamos Sequential, un modelo lineal para apilar capas de red neuronal
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout   # Importamos capas espec√≠ficas para construir una red neuronal convolucional
from tf.keras.optimizers import SGD
from tensorflow.keras.callbacks import LearningRateScheduler
import time


def reparte(numero: int, nr_partes: int) -> list[int]:
    """Divide `numero` en `nr_partes` partes enteras más o menos iguales.
    Por ejemplo, reparte(10, 3) devuelve la lista [4, 3, 3]."""
    cociente, resto = divmod(numero, nr_partes)
    return [cociente + 1] * resto + [cociente] * (nr_partes - resto)

def generar_listas(lista, totales):
    resultado = []
    inicio = 0

    for longitud in lista:
        nueva_lista = totales[inicio:longitud+1]
        resultado.append(nueva_lista)
        totales = totales[longitud+1:]

    return resultado


def concat(xss):
    xs = []
    for i in xss:
        xs += i
    return xs

def first(par):
    return par[0]

def second(par):
    return par[1]

def leer_csv(name):

    # Diccionario para almacenar id y label
    diccionario_id_label = {}

    # Leer el archivo CSV
    with open(name, newline='') as csvfile:
        # Crear un objeto lector CSV
        lector = csv.DictReader(csvfile)

        # Iterar sobre las filas del archivo CSV
        for fila in lector:
            # Obtener valores de las columnas "Id" y "Class"
            id_valor = fila['Id']  
            label_valor = fila['Class']

            # Almacenar en el diccionario
            diccionario_id_label[id_valor] = label_valor
    return diccionario_id_label


def verificar_extension(archivo):
    # Obtener la extensión del archivo
    _, extension = os.path.splitext(archivo)

    # Comparar la extensión con '.bytes' o '.asm'
    if extension.lower() == '.bytes':
        return True
    else:
        return False
    

def visualize_image(image_matrix):
    plt.imshow(image_matrix, cmap='gray')
    plt.axis('off')
    plt.show()
    
    
def metodo1(width, height, image_array):
    
    lon = len(image_array)
    lado = floor(sqrt(lon))
    image_array = image_array[:lado*lado]
    image_matrix = image_array.reshape((lado, lado))
    
    copy = np.copy(image_matrix, order='C')
    grayscale_image = np.resize(copy, (width, height)).astype(np.uint8)
    
    return grayscale_image
    
    
def metodo2(width, height, image_array):
    # Convertir el array NumPy a una imagen PIL
    image = Image.fromarray(image_array.astype(np.uint8))

    # Obtener las dimensiones originales de la imagen
    originalW, originalH = image.size
    
    if originalW <= width + 1 or originalH <= height + 1:
        imagen = metodo1(width,height,image_array)
    else:
        # Calcular las coordenadas del recorte aleatorio
        left = np.random.randint(0, originalW - width + 1)
        top = np.random.randint(0, originalH - height + 1)
        right = left + width
        bottom = top + height
    
        # Aplicar el recorte aleatorio a la imagen
        recorte = image.crop((left, top, right, bottom))
    
        # Convertir la imagen recortada de nuevo a un array NumPy
        cropped_array = np.array(recorte)
        
        imagen = cropped_array.reshape((width,height))

    return imagen


def metodo3(width, height, image_array, interpolation):
    
    lon = len(image_array)
    lado = floor(sqrt(lon))
    if lado == 0:
        return np.empty((0,0))
    
    lado2 = lon // lado
    image_array = image_array[:lado*lado2]
    image_matrix2 = image_array.reshape((lado, lado2))
    
    image_matrix2 = image_matrix2.astype(np.uint8)
    grayscale_image2 = cv2.resize(image_matrix2, (width, height), interpolation = interpolation)
    
    return grayscale_image2
    
def bytes_to_image(byte_str, width, height, interpolation, tipo):
    byte_list = [int(byte, 16) for line in byte_str.split('\n') for byte in line.split()[1:]]
    image_array = np.array(byte_list)
    
    if tipo == 1:
        imagen = metodo1(width, height, image_array)
    elif tipo == 2:
        imagen = metodo2(width, height, image_array)
    else:
        imagen = metodo3(width, height, image_array, interpolation)

    # Expande las dimensiones para indicar que es una imagen en escala de grises
    result = np.stack([imagen] * 3, axis=-1)
    
    
    return result


    

def file_to_image(name, interpolation, tipo):

    # Read lines from the file
    ruta = os.path.join('train2')
    ruta_archiv = os.path.join(ruta,name)
    lines = open(ruta_archiv, "r").readlines()

    # Format lines into a string
    malware_bytes = """ """ + "\n".join(lines)

    malware_bytes = "\n".join(line for line in malware_bytes.split("\n") if "?" not in line)

    # Set the desired width and height for the image
    image_width = 224
    image_height = 224

    # Convert the .bytes file snippet to a grayscale image
    malware_image = bytes_to_image(malware_bytes, image_width, image_height, interpolation, tipo)

    # Visualize the image
    visualize_image(malware_image)
    return malware_image



# Definimos una funci√≥n para cargar y procesar datos
def load_and_preprocess_data(ruta, trainLabel, images, interpolation = cv2.INTER_LINEAR, tipo = 3):

    data = []                                         # Lista para almacenar datos de im√°genes
    labels = []                                       # Lista para almacenar etiquetas de im√°genes                         
    
    for i in images:  
        imagen = file_to_image(i, interpolation, tipo)
        if imagen.size == 0:
            print(i)
            pass
        else:
            data.append(imagen)
            Id = os.path.splitext(i)[0]
            label = trainLabel[Id]
            labels.append(label)

    return data, labels
    


def load_and_preprocess_data_paralelo(nr_procesos, interpolation = cv2.INTER_LINEAR, tipo = 3):
    p = mp.Pool(nr_procesos)
    
    ruta = os.path.join('train2')
    images_tot = os.listdir(ruta)
    images_tot = list(filter(verificar_extension, images_tot))
    trainLabel = leer_csv('trainLabels.csv')
    
    reparto_doc = reparte(len(images_tot), nr_procesos)
    archivos = generar_listas(reparto_doc, images_tot)
    args_list = [(ruta,trainLabel,k, interpolation, tipo) for k in archivos]
    resultados = p.starmap(load_and_preprocess_data, args_list)
    
    data = concat(list(map(first,resultados)))
    label = concat(list(map(second,resultados)))
    label = np.array(label)
    data = np.array(data)
    
    return data, label



if __name__ == "__main__":
    t1 = perf_counter()
    tipo = 1
    interpolation = cv2.INTER_LINEAR
    resultados = load_and_preprocess_data_paralelo(6, interpolation, tipo)
    with open('resultados1.pkl', 'wb') as f:
        pickle.dump(resultados, f)
    t2 = perf_counter()
    print(t2-t1)
'''
    ruta = os.path.join('train')
    images_tot = os.listdir(ruta)
    images_tot = list(filter(verificar_extension, images_tot))
    trainLabel = leer_csv('trainLabels.csv')
    
    archivos = images_tot
    t1 = perf_counter()
    resultado = load_and_preprocess_data(ruta, trainLabel, archivos)
    t2 = perf_counter()
    print(f'Tiempo {t2-t1}')
    for i in range(1,9):
        t1 = perf_counter()
        resultados = load_and_preprocess_data_paralelo(i) 
        t2 = perf_counter()
        print(f'Tiempo {t2-t1}')
'''








# Definimos una funci√≥n para dividir y codificar los datos
def split_and_encode_data(data, labels, test_size=0.1, random_state=10):
    # Dividimos los datos en conjuntos de entrenamiento y prueba, y codificamos las etiquetas en formato one-hot
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=random_state) # Dividimos los conjuntos de datos data(array de dimensiones (39209, 30, 30, 3) (numero de muestras, dimension imagenes y numero de canales de colores RGB) y labels en conjunto de entrenamiento y de prueba. Las x hacen referencia a los datos y las y a las etiquetas. Le ponemos que se reparta en %. El porcentaje de datos para el test es el numero que le introduzcamos como parámatro de entrada.
    y_train -= 1                          # Las clases van de 1 a 9, pero la codificacion one-hot es de 0 a n-1 luego tenemos que reetiquetar cada imagen
    y_test -= 1
    y_test = to_categorical(y_test, 9)    # Codificamos en one-hot las etiquetas de prueba
    y_train = to_categorical(y_train, 9)  # Codificamos en one-hot las etiquetas de entrenamiento
    return X_train, X_test, y_train, y_test

# Definimos una funci√≥n para construir el modelo de red neuronal convolucional
def build_model(input_shape):
    model = Sequential()

    # Primera capa: 2 capas convolution + relu 224x224x64
    model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding="same", activation="relu"))
    model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
    
    # Segunda capa: 1 max pooling 112x112x128
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

    # Tercera capa: 2 capas convolution + relu 112x112x128
    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
    
    # Cuarta capa: 1 max pooling 56x56x256
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    
    # Quinta capa: 3 capas convolution + relu 56x56x256
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
    
    # Sexta capa: 1 max pooling 28x28x512
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    
    # Séptima capa: 3 capas convolution + relu 28x28x512
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    
    # Octava capa: 1 max pooling 14x14x512
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    
    # Novena capa: 3 capas convolution + relu 14x14x512
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
    
    # Décima capa: 1 max pooling 7x7x512
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    
    # Undécima capa: 2 fully connected + relu 1x1x4096
    model.add(Flatten())
    model.add(Dense(units=4096,activation="relu"))
    model.add(Dropout(0.5))
    model.add(Dense(units=4096,activation="relu"))
    model.add(Dropout(0.5))
    #model.add(Dense(units=9, activation='relu'))
    #model.add(Dropout(0.5))
    model.add(Dense(units=9, activation='softmax'))

    model.summary()
    return model


def step_decay(epoch):
   initial_lrate = 0.001
   drop = 0.1
   epochs_drop = 20.0
   lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
   return lrate
'''
def train_model(model, file_name, X_train, y_train, X_test, y_test, batch_size=8, epochs=25):
    initial_learning_rate = 0.001
    momentum = 0.9

    # Learning rate schedule
    lr_schedule = LearningRateScheduler(step_decay)
    
    # SGD optimizer with weight decay and momentum
    sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=momentum, weight_decay=0.0005)

    # Compile the model
    model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    with tf.device('/GPU:0'):
        # Training the model with learning rate schedule
        model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[lr_schedule],shuffle=True)

    # Save the model
    model.save(file_name)
'''
# Custom callback to record training time
class TimeHistory(tf.keras.callbacks.Callback):
        def on_train_begin(self, logs={}):
            self.times = []

        def on_epoch_begin(self, epoch, logs={}):
            self.epoch_time_start = time.time()

        def on_epoch_end(self, epoch, logs={}):
            self.times.append(time.time() - self.epoch_time_start)

def train_model(model, file_name, X_train, y_train, X_test, y_test, batch_size=8, epochs=25):
    initial_learning_rate = 0.001
    momentum = 0.9

    # Learning rate schedule
    lr_schedule = LearningRateScheduler(step_decay)
    
    # SGD optimizer with weight decay and momentum
    sgd_optimizer = SGD(learning_rate=initial_learning_rate, momentum=momentum, weight_decay=0.0005)

    # Compile the model
    model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # Initialize lists to store history
    train_loss = []
    train_acc = []
    val_loss = []
    val_acc = []
    times = []

    time_callback = TimeHistory()

    with tf.device('/GPU:0'):
        # Training the model with learning rate schedule
        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[lr_schedule, time_callback], shuffle=True)

    # Record history
    train_loss = history.history['loss']
    train_acc = history.history['accuracy']
    val_loss = history.history['val_loss']
    val_acc = history.history['val_accuracy']
    times = time_callback.times

    # Save history to CSV file
    with open(file_name, 'w', newline='') as csvfile:
        fieldnames = ['epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc', 'time']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for i in range(len(train_loss)):
            writer.writerow({'epoch': i+1, 'train_loss': train_loss[i], 'train_acc': train_acc[i], 'val_loss': val_loss[i], 'val_acc': val_acc[i], 'time': times[i]})

    # Save the model
    model.save(file_name + '.keras')

    return history

'''
La diferencia principal entre realizar el entrenamiento del modelo en la GPU o en la CPU radica en el rendimiento y la velocidad de entrenamiento.

GPU (Unidad de Procesamiento Gr√°fico):

Ventajas:
Las GPU est√°n dise√±adas espec√≠ficamente para manejar operaciones matriciales y paralelas, que son comunes en el entrenamiento de modelos de redes neuronales.
Pueden realizar c√°lculos en paralelo en grandes cantidades de datos, lo que acelera significativamente el entrenamiento de modelos, especialmente en tareas intensivas en c√°lculos, como las redes neuronales profundas.
Ofrecen un rendimiento superior en comparaci√≥n con las CPU para tareas de aprendizaje profundo.

Desventajas:
Pueden ser m√°s costosas y consumir m√°s energ√≠a que las CPU.
Puede haber limitaciones en la cantidad de memoria de la GPU disponible, lo que podr√≠a ser un factor en modelos muy grandes.

CPU (Unidad Central de Procesamiento):

Ventajas:
Disponibles en la mayor√≠a de las computadoras y servidores sin necesidad de hardware adicional.
Adecuadas para tareas generales de prop√≥sito m√∫ltiple y no solo para aprendizaje profundo.
Pueden ser m√°s econ√≥micas en t√©rminos de hardware y consumo de energ√≠a.
Desventajas:
Las CPU no est√°n dise√±adas espec√≠ficamente para tareas de aprendizaje profundo y pueden ser menos eficientes en t√©rminos de velocidad para ciertos tipos de operaciones, especialmente en modelos grandes.

'''

if __name__ == "__main__":
    
    with open('resultados.pkl', 'rb') as f:
        data,labels = pickle.load(f)
    # Calcular la cantidad de datos por etiqueta
    labels = labels.astype(np.int64)
    
    '''
    # Cargamos los datos
    data, labels = load_and_preprocess_data_paralelo(5) 
    
    
    
    # Crear un gr√°fico de barras con etiquetas
    plt.bar(range(len(labels)), labels, tick_label=range(len(labels)))

    # Agregar etiquetas con los n√∫meros en cada barra
    for i, count in enumerate(labels):
        plt.text(i, count + 0.1, str(count), ha='center', va='bottom')

    plt.xlabel('Etiquetas')
    plt.ylabel('Cantidad de Datos')
    plt.title('Cantidad de Datos por Etiqueta')
    plt.show()
    '''
    
    # Dividimos los datos
    X_train, X_test, y_train, y_test = split_and_encode_data(data, labels)
       

    # Construimos el modelo de CNN
    model = build_model(X_train.shape[1:]) #.shape te dice las dimensiones y caracteristicas de X_train. La primera indica el numero que hay, las siguientes las dimensiones y los canales de colores (que es lo que nos interesa)
    
    # Entrenamos el modelo y lo guardamos en un archivo
    t1 = perf_counter()
    train_model(model, 'malware_classifier_reduccLR20_dropout05_16batch', X_train, y_train, X_test, y_test, 16, 25)
    t2 = perf_counter()
    print(f"EL tiempo que ha tardado en cargarse el modelo es: {(((t2-t1) //60) // 60)} horas, {(((t2-t1) //60) % 60) } minutos y {(t2-t1) % 60 } segundos") 




"""
Para predecir la se√±al que aparece en una imagen, tenemos que cargar el modelo despues de 20 epochs y
la imagen. Esta √∫ltima tenemos que pasarla a una imagen con tres canales  y redimensionarla a 30 x 30 pixeles.
Despues de que mi modelo prediga cual ser√≠a su etiqueta, le asignamos su nombre correspondiente del diccionario 
clases.
"""

def predecir(model, nombre):
    os.getcwd()
    # Cargar el modelo previamente entrenado
    model = load_model(model)
    
    # Cargar la imagen
    image_path = nombre 
    image0 = file_to_image(image_path)
    image3 = np.array(image0)
    # Normalizar los valores de p√≠xeles
    
    # Realizar la predicci√≥n con el modelo cargado
    prediction = model.predict(np.array([image3]))  # Asegurarse de que sea un arreglo de forma (1, 30, 30, 3)
    
    # Obtener la etiqueta predicha (√≠ndice de la clase con mayor probabilidad)
    predicted_class = np.argmax(prediction)
    
    # Crear un diccionario que mapea las clases a sus etiquetas
    clases = { 
        1: 'Ramnit', 
        2: 'Lollipop', 
        3: 'Kelihos_ver3', 
        4: 'Vundo', 
        5: 'Simda', 
        6: 'Tracur', 
        7: 'Kelihos_ver1', 
        8: 'Obfuscator.ACY', 
        9: 'Gatak'
    }
    
    # Obtener la etiqueta correspondiente a la clase predicha
    predicted_label = clases[predicted_class+1]
    
    probabilities = prediction[0]
    
    # Imprimir la etiqueta predicha
    print(f'Clase predicha: {predicted_label}')
    
    print('Probabilidades:')
    for i, prob in enumerate(probabilities):
        print(f'{clases[i+1]}: {prob:.4f}')
        
        

   
