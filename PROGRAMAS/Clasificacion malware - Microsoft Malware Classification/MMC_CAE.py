"""
Nombre del codigo: Modelo de un Convolucional Autoencoder para clasificación de malware.
Base de datos: Microsoft Malware Dataset
Alumno: Jim√©nez Poyatos, Pablo

"""

import pickle
import tensorflow as tf
import numpy as np

from MMC_CNN import step_decay
from tensorflow.keras import layers, models, Model
from keras.models import load_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, EarlyStopping
from confusionMMC_CNN import confusion_matrix_pablo
from aux import split_and_encode_data


def build_conv_autoencoder(input_shape) -> Model:
    """
    Construye un autoencoder convolucional.

    Args:
        input_shape (tuple): La forma de entrada de las imágenes (altura, anchura, canales).

    Returns:
        models.Model: Un modelo de autoencoder convolucional de Keras.

    Input:
        - input_shape: Una tupla que representa las dimensiones de la entrada (altura, anchura, canales).

    Output:
        - Un modelo de autoencoder convolucional de Keras.
    """
    # Encoder
    input_layer = layers.Input(shape=input_shape)

    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 112x112x32

    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 56x56x64

    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 28x28x128

    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # 14x14x256

    encoded = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)
    encoded = layers.MaxPooling2D((2, 2), padding='same')(encoded)  # 7x7x512

    # Decoder
    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(encoded)
    x = layers.UpSampling2D((2, 2))(x)  # 14x14x512

    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = layers.UpSampling2D((2, 2))(x)  # 28x28x256

    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = layers.UpSampling2D((2, 2))(x)  # 56x56x128

    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.UpSampling2D((2, 2))(x)  # 112x112x64

    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = layers.UpSampling2D((2, 2))(x)  # 224x224x32

    decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)  # 224x224x3

    autoencoder = models.Model(input_layer, decoded)

    autoencoder.summary()

    return autoencoder

def build_classification_model(encoder, input_shape, num_classes) -> Model:
    """
    Construye un modelo de clasificación utilizando un encoder preentrenado.

    Args:
        encoder (models.Model): Un modelo preentrenado que actúa como el encoder.
        input_shape (tuple): La forma de entrada para el modelo de clasificación.
        num_classes (int): El número de clases para la salida de clasificación.

    Returns:
        models.Model: Un modelo de clasificación compilado.

    Input:
        - encoder: Un modelo de Keras que actúa como el encoder.
        - input_shape: Una tupla que representa las dimensiones de la entrada (altura, anchura, canales).
        - num_classes: Un entero que representa el número de clases de salida.

    Output:
        - Un modelo de clasificación de Keras.
    """
    # Congelar las capas del encoder
    for layer in encoder.layers:
        print(layer)
        layer.trainable = False

    # Añadir capas de clasificación
    classification_input = layers.Input(shape=input_shape)
    x = encoder(classification_input)
    x = layers.Flatten()(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    classification_output = layers.Dense(num_classes, activation='softmax')(x)

    classification_model = models.Model(classification_input, classification_output)
    classification_model.summary()

    return classification_model


if __name__ == "__main__":
    # Hiperparámetros iniciales
    file_name = "MMC_autoencoder"
    input_shape = (224, 224, 3)
    epochsAE = 30
    batch_sizeAE = 8
    epochsC = 25
    batch_sizeC = 8
    num_classes = 9
    csv_logger = CSVLogger('autoencoder_training_log_ADAM_MSE.csv', append=False)
    
    # Procesamiento de datos
    with open('eliminarL.pkl', 'rb') as f:
        data, labels = pickle.load(f)
    labels = labels.astype(np.int64)

    with open('eliminarL_test.pkl', 'rb') as f:
        data2 = pickle.load(f)
        
    X_trainC, X_modeloC, y_trainC, y_modeloC = split_and_encode_data(data, labels, test_size=0.25, random_state=0)
    X_valC, X_testC, y_valC, y_testC = train_test_split(X_modeloC, y_modeloC, test_size=0.6, random_state=1)
    X_train, X_val = train_test_split(data2, test_size=0.2, random_state=2)

    X_train = np.concatenate([X_trainC, X_train], axis=0)
    X_val = np.concatenate([X_val, X_valC], axis=0)

    X_train = X_train.astype('float32') / 255.
    X_val = X_val.astype('float32') / 255.
    X_trainC = X_trainC.astype('float32') / 255.
    X_valC = X_valC.astype('float32') / 255.
    X_testC = X_testC.astype('float32') / 255.


    # Construir y entrenar el autoencoder
    autoencoder = build_conv_autoencoder(input_shape)
    autoencoder.compile(optimizer='adam', loss='mse')
    autoencoder.fit(X_train, X_train, epochs=epochsAE, batch_size=batch_sizeAE, validation_data=(X_val,X_val),
                    callbacks=[csv_logger], shuffle=True)
    autoencoder.save(file_name + '_cae_adam_mse.keras')


    # Contruir y entrenar clasificación
    initial_learning_rate = 0.001
    momentum = 0.9
    lr_schedule = LearningRateScheduler(step_decay)
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=4,
                                   restore_best_weights=True, start_from_epoch=20)
    csv_logger_clas = CSVLogger('autoencoder_training_clas_SGD_CE', append=False)

    autoencoder = load_model('MMC_autoencoder_cae_100.keras')
    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[10].output)  # El índice 10 corresponde a la última capa del encoder
    classification_model = build_classification_model(encoder, input_shape, num_classes)

    sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=momentum, weight_decay=0.0005)
    classification_model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    classification_model.fit(X_trainC, y_trainC, batch_size=batch_sizeC, epochs=epochsC, validation_data=(X_valC, y_valC),
                            callbacks=[early_stopping, csv_logger_clas], shuffle=True)

    classification_model.save(file_name + '.keras')


    # Evaluación del modelo con los datos de test
    confusion_matrix_pablo('autoencoder/MMC_autoencoder_CAE_SGD.keras', X_testC, y_testC, 'confusionMatrixCAE')



