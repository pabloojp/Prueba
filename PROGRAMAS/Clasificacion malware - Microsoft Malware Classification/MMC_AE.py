"""
Nombre del codigo: Modelo de un Autoencoder simple para clasificación de malware.
Base de datos: Microsoft Malware Dataset
Alumno: Jim√©nez Poyatos, Pablo

"""
from aux import split_and_encode_data, guardar_informacion, TimeHistory
from MMC_CNN import train_model, step_decay
from tensorflow.keras import layers, models, Model
from tensorflow.keras.optimizers import SGD
from keras.models import load_model
import tensorflow as tf
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, EarlyStopping


def build_autoencoder(input_shape: int) -> models.Model:
    """
    Construye un autoencoder simple.

    Args:
        input_shape (int): La forma de entrada para la capa de entrada del autoencoder.

    Returns:
        models.Model: Un modelo de autoencoder compilado.

    Input:
        - input_shape: Un entero que representa la dimensión de la entrada.

    Output:
        - Un modelo de autoencoder de Keras.
    """
    # Encoder
    input_layer = layers.Input(shape=(input_shape,))
    encoded = layers.Dense(4096, activation='relu')(input_layer)  # Cuello de botella

    # Decoder
    decoded = layers.Dense(input_shape, activation='sigmoid')(encoded)

    autoencoder = models.Model(input_layer, decoded)

    autoencoder.summary()

    return autoencoder


def build_classification_model(encoder: models.Model, input_shape: int, num_classes: int) -> models.Model:
    """
    Construye un modelo de clasificación utilizando un encoder preentrenado.

    Args:
        encoder (models.Model): Un modelo preentrenado que actúa como el encoder.
        input_shape (int): La forma de entrada para el modelo de clasificación.
        num_classes (int): El número de clases para la salida de clasificación.

    Returns:
        models.Model: Un modelo de clasificación compilado.

    Input:
        - encoder: Un modelo de Keras que actúa como el encoder.
        - input_shape: Un entero que representa la dimensión de la entrada.
        - num_classes: Un entero que representa el número de clases de salida.

    Output:
        - Un modelo de clasificación de Keras.
    """
    # Congelar las capas del encoder
    for layer in encoder.layers:
        layer.trainable = False

    # Añadir capas de clasificación
    classification_input = layers.Input(shape=(input_shape,))
    x = encoder(classification_input)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    classification_output = layers.Dense(num_classes, activation='softmax')(x)  # Suponiendo num_classes clases

    classification_model = models.Model(classification_input, classification_output)

    classification_model.summary()

    return classification_model


if __name__ == "__main__":
    # Hiperparámetros iniciales
    file_name = "MMC_autoencoder_AE_clasificacion_CE_2"
    input_shape = 50176
    epochsAE = 14
    batch_sizeAE = 8
    epochsC = 25
    batch_sizeC = 8
    num_classes = 9
    csv_logger = CSVLogger('autoencoder_training_AE_extra.csv', append=False)


    # Preprocesar los datos
    with open('eliminarL.pkl', 'rb') as f:
        data, labels = pickle.load(f)
    labels = labels.astype(np.int64)

    with open('eliminarL_test.pkl', 'rb') as f:
        data2 = pickle.load(f)
        
    X_trainC, X_modeloC, y_trainC, y_modeloC = split_and_encode_data(data, labels, test_size=0.25, random_state=0)
    X_valC, X_testC, y_valC, y_testC = train_test_split(X_modeloC, y_modeloC, test_size=0.6, random_state=1)
    
    X_train, X_val = train_test_split(data2, test_size=0.2, random_state=2)
    X_train = np.concatenate([X_trainC, X_train], axis=0)
    X_val = np.concatenate([X_val, X_valC], axis=0)

    X_train = np.array([x.reshape(50176, -1) for x in ((X_train.astype('float32') / 255.0)[:,:,:,0])])
    X_val = np.array([x.reshape(50176, -1) for x in ((X_val.astype('float32') / 255.0)[:,:,:,0])])
    X_trainC = np.array([x.reshape(50176, -1) for x in ((X_trainC.astype('float32') / 255.0)[:,:,:,0])])
    X_valC = np.array([x.reshape(50176, -1) for x in ((X_valC.astype('float32') / 255.0)[:,:,:,0])])
    X_testC = np.array([x.reshape(50176, -1) for x in ((X_testC.astype('float32') / 255.0)[:,:,:,0])])
    

    # Crear modelo de autoencoder y entrenamiento
    autoencoder = build_autoencoder(input_shape)
    autoencoder.compile(optimizer='adam', loss='mse')
    autoencoder.fit(X_train, X_train, epochs=epochsAE, batch_size=batch_sizeAE, validation_data=(X_val,X_val), callbacks=[csv_logger], shuffle=True)
    autoencoder.save(file_name + '_ae.keras')


    # Crear la clasificación con la representación comprimida de los datos.
    initial_learning_rate = 0.001
    momentum = 0.9
    lr_schedule = LearningRateScheduler(step_decay)
    csv_logger_clas = CSVLogger('autoencoder_training_clas_ae_SGD', append=False)
    early_stopping = EarlyStopping(monitor='val_loss', patience=4,
                                   restore_best_weights=True)

    autoencoder = load_model("MMC_autoencoder_ae.keras")
    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)

    sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=momentum, weight_decay=0.0005)
    classification_model = build_classification_model(encoder, input_shape, num_classes)

    classification_model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy' , metrics=['accuracy'])
    classification_model.fit(X_trainC, y_trainC, batch_size=batch_sizeC, epochs=epochsC, validation_data=(X_valC, y_valC),
                            callbacks=[csv_logger_clas], shuffle=True)

    classification_model.save(file_name + '.keras')
    
